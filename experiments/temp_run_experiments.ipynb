{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "devices = 'auto'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the directory containing your script to sys.path\n",
    "script_directory = os.path.dirname('/home/bethge/bkr046/CBM-intervention-concept-correction/experiments/temp_run_experiments.ipynb')\n",
    "sys.path.insert(0, script_directory + '/../')\n",
    "\n",
    "#####\n",
    "\n",
    "import argparse\n",
    "import copy\n",
    "import joblib\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "from cem.data.synthetic_loaders import (\n",
    "    get_synthetic_data_loader, get_synthetic_num_features\n",
    ")\n",
    "import cem.data.celeba_loader as celeba_data_module\n",
    "import cem.data.chexpert_loader as chexpert_data_module\n",
    "import cem.data.CUB200.cub_loader as cub_data_module\n",
    "import cem.data.derm_loader as derm_data_module\n",
    "import cem.data.mnist_add as mnist_data_module\n",
    "import cem.interventions.utils as intervention_utils\n",
    "import cem.train.training as training\n",
    "import cem.train.utils as utils\n",
    "\n",
    "from experiment_utils import (\n",
    "    evaluate_expressions, determine_rerun,\n",
    "    generate_hyperatemer_configs, filter_results,\n",
    "    print_table, get_mnist_extractor_arch\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "from run_experiments import _build_arg_parser\n",
    "parser = _build_arg_parser()\n",
    "args = parser.parse_args('-c /home/bethge/bkr046/CBM-intervention-concept-correction/experiments/configs/intcem_configs/cub.yaml'.split(' '))\n",
    "\n",
    "if args.project_name:\n",
    "    # Lazy import to avoid importing unless necessary\n",
    "    pass #import wandb\n",
    "if args.debug:\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "else:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n",
    "\n",
    "if args.config:\n",
    "    with open(args.config, \"r\") as f:\n",
    "        loaded_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "else:\n",
    "    loaded_config = {}\n",
    "if \"shared_params\" not in loaded_config:\n",
    "    loaded_config[\"shared_params\"] = {}\n",
    "if \"runs\" not in loaded_config:\n",
    "    loaded_config[\"runs\"] = []\n",
    "\n",
    "if args.dataset is not None:\n",
    "    loaded_config[\"dataset\"] = args.dataset\n",
    "if loaded_config.get(\"dataset\", None) is None:\n",
    "    raise ValueError(\n",
    "        \"A dataset must be provided either as part of the \"\n",
    "        \"configuration file or as a command line argument.\"\n",
    "    )\n",
    "if loaded_config[\"dataset\"] == \"cub\":\n",
    "    data_module = cub_data_module\n",
    "    args.project_name = args.project_name.format(ds_name=\"cub\")\n",
    "elif loaded_config[\"dataset\"] == \"derm\":\n",
    "    data_module = derm_data_module\n",
    "    args.project_name = args.project_name.format(ds_name=\"derma\")\n",
    "elif loaded_config[\"dataset\"] == \"celeba\":\n",
    "    data_module = celeba_data_module\n",
    "    args.project_name = args.project_name.format(ds_name=\"celeba\")\n",
    "elif loaded_config[\"dataset\"] == \"chexpert\":\n",
    "    data_module = chexpert_data_module\n",
    "    args.project_name = args.project_name.format(ds_name=\"chexpert\")\n",
    "elif loaded_config[\"dataset\"] in [\"xor\", \"vector\", \"dot\", \"trig\"]:\n",
    "    data_module = get_synthetic_data_loader(loaded_config[\"dataset\"])\n",
    "    args.project_name = args.project_name.format(\n",
    "        ds_name=loaded_config[\"dataset\"]\n",
    "    )\n",
    "    input_features = get_synthetic_num_features(loaded_config[\"dataset\"])\n",
    "    def synth_c_extractor_arch(\n",
    "        output_dim,\n",
    "        pretrained=False,\n",
    "    ):\n",
    "        if output_dim is None:\n",
    "            output_dim = 128\n",
    "        return torch.nn.Sequential(*[\n",
    "            torch.nn.Linear(input_features, 128),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(128, 128),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(128, output_dim),\n",
    "        ])\n",
    "    loaded_config[\"c_extractor_arch\"] = synth_c_extractor_arch\n",
    "elif loaded_config[\"dataset\"] == \"mnist_add\":\n",
    "    data_module = mnist_data_module\n",
    "    args.project_name = args.project_name.format(ds_name=args.dataset)\n",
    "    utils.extend_with_global_params(\n",
    "        loaded_config,\n",
    "        args.param or []\n",
    "    )\n",
    "    num_operands = loaded_config.get('num_operands', 32)\n",
    "    loaded_config[\"c_extractor_arch\"] = get_mnist_extractor_arch(\n",
    "        input_shape=(\n",
    "            loaded_config.get('batch_size', 512),\n",
    "            num_operands,\n",
    "            28,\n",
    "            28,\n",
    "        ),\n",
    "        num_operands=num_operands,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported dataset {loaded_config['dataset']}!\")\n",
    "\n",
    "if args.output_dir is not None:\n",
    "    loaded_config['results_dir'] = args.output_dir\n",
    "if args.debug:\n",
    "    print(json.dumps(loaded_config, sort_keys=True, indent=4))\n",
    "logging.info(f\"Results will be dumped in {loaded_config['results_dir']}\")\n",
    "logging.debug(\n",
    "    f\"And the dataset's root directory is {loaded_config.get('root_dir')}\"\n",
    ")\n",
    "Path(loaded_config['results_dir']).mkdir(parents=True, exist_ok=True)\n",
    "# Write down the actual command executed\n",
    "# And the configuration file\n",
    "now = datetime.now()\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%Y_%m_%d_%H_%M\")\n",
    "loaded_config[\"time_last_called\"] = now.strftime(\"%Y/%m/%d at %H:%M:%S\")\n",
    "with open(\n",
    "    os.path.join(loaded_config['results_dir'], f\"command_{dt_string}.txt\"),\n",
    "    \"w\",\n",
    ") as f:\n",
    "    command_args = [\n",
    "        arg if \" \" not in arg else f'\"{arg}\"' for arg in sys.argv\n",
    "    ]\n",
    "    f.write(\"python \" + \" \".join(command_args))\n",
    "\n",
    "# Also save the current experiment configuration\n",
    "with open(\n",
    "    os.path.join(\n",
    "        loaded_config['results_dir'],\n",
    "        f\"experiment_{dt_string}_config.yaml\")\n",
    "    ,\n",
    "    \"w\"\n",
    ") as f:\n",
    "    yaml.dump(loaded_config, f)\n",
    "\n",
    "\n",
    "data_module=data_module\n",
    "rerun=args.rerun\n",
    "result_dir=(\n",
    "    args.output_dir if args.output_dir\n",
    "    else loaded_config['results_dir']\n",
    ")\n",
    "project_name=args.project_name\n",
    "num_workers=args.num_workers\n",
    "global_params=args.param\n",
    "accelerator=(\n",
    "    \"gpu\" if (not args.force_cpu) and (torch.cuda.is_available())\n",
    "    else \"cpu\"\n",
    ")\n",
    "experiment_config=loaded_config\n",
    "activation_freq=args.activation_freq\n",
    "single_frequency_epochs=args.single_frequency_epochs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "seed_everything(42)\n",
    "# parameters for data, model, and training\n",
    "experiment_config = copy.deepcopy(experiment_config)\n",
    "if 'shared_params' not in experiment_config:\n",
    "    experiment_config['shared_params'] = {}\n",
    "# Move all global things into the shared params\n",
    "for key, vals in experiment_config.items():\n",
    "    if key not in ['runs', 'shared_params']:\n",
    "        experiment_config['shared_params'][key] = vals\n",
    "experiment_config['shared_params']['num_workers'] = num_workers\n",
    "\n",
    "utils.extend_with_global_params(\n",
    "    experiment_config['shared_params'], global_params or []\n",
    ")\n",
    "\n",
    "train_dl, val_dl, test_dl, imbalance, (n_concepts, n_tasks, concept_map) = \\\n",
    "    data_module.generate_data(\n",
    "        config=experiment_config['shared_params'],\n",
    "        seed=42,\n",
    "        output_dataset_vars=True,\n",
    "        root_dir=experiment_config['shared_params'].get('root_dir', None),\n",
    "    )\n",
    "\n",
    "# For now, we assume that all concepts have the same\n",
    "# aquisition cost\n",
    "acquisition_costs = None\n",
    "if concept_map is not None:\n",
    "    intervened_groups = list(\n",
    "        range(\n",
    "            0,\n",
    "            len(concept_map) + 1,\n",
    "            experiment_config['shared_params'].get('intervention_freq', 1),\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    intervened_groups = list(\n",
    "        range(\n",
    "            0,\n",
    "            n_concepts + 1,\n",
    "            experiment_config['shared_params'].get('intervention_freq', 1),\n",
    "        )\n",
    "    )\n",
    "experiment_config[\"shared_params\"][\"n_concepts\"] = \\\n",
    "    experiment_config[\"shared_params\"].get(\n",
    "        \"n_concepts\",\n",
    "        n_concepts,\n",
    "    )\n",
    "experiment_config[\"shared_params\"][\"n_tasks\"] = \\\n",
    "    experiment_config[\"shared_params\"].get(\n",
    "        \"n_tasks\",\n",
    "        n_tasks,\n",
    "    )\n",
    "experiment_config[\"shared_params\"][\"concept_map\"] = \\\n",
    "    experiment_config[\"shared_params\"].get(\n",
    "        \"concept_map\",\n",
    "        concept_map,\n",
    "    )\n",
    "\n",
    "sample = next(iter(train_dl))\n",
    "real_sample = []\n",
    "for x in sample:\n",
    "    if isinstance(x, list):\n",
    "        real_sample += x\n",
    "    else:\n",
    "        real_sample.append(x)\n",
    "sample = real_sample\n",
    "logging.info(\n",
    "    f\"Training sample shape is: {sample[0].shape} with \"\n",
    "    f\"type {sample[0].type()}\"\n",
    ")\n",
    "logging.info(\n",
    "    f\"Training label shape is: {sample[1].shape} with \"\n",
    "    f\"type {sample[1].type()}\"\n",
    ")\n",
    "logging.info(\n",
    "    f\"\\tNumber of output classes: {n_tasks}\"\n",
    ")\n",
    "logging.info(\n",
    "    f\"Training concept shape is: {sample[2].shape} with \"\n",
    "    f\"type {sample[2].type()}\"\n",
    ")\n",
    "logging.info(\n",
    "    f\"\\tNumber of training concepts: {n_concepts}\"\n",
    ")\n",
    "\n",
    "task_class_weights = None\n",
    "\n",
    "if experiment_config['shared_params'].get('use_task_class_weights', False):\n",
    "    logging.info(\n",
    "        f\"Computing task class weights in the training dataset with \"\n",
    "        f\"size {len(train_dl)}...\"\n",
    "    )\n",
    "    attribute_count = np.zeros((max(n_tasks, 2),))\n",
    "    samples_seen = 0\n",
    "    for i, data in enumerate(train_dl):\n",
    "        if len(data) == 2:\n",
    "            (_, (y, _)) = data\n",
    "        else:\n",
    "            (_, y, _) = data\n",
    "        if n_tasks > 1:\n",
    "            y = torch.nn.functional.one_hot(\n",
    "                y,\n",
    "                num_classes=n_tasks,\n",
    "            ).cpu().detach().numpy()\n",
    "        else:\n",
    "            y = torch.cat(\n",
    "                [torch.unsqueeze(1 - y, dim=-1), torch.unsqueeze(y, dim=-1)],\n",
    "                dim=-1,\n",
    "            ).cpu().detach().numpy()\n",
    "        attribute_count += np.sum(y, axis=0)\n",
    "        samples_seen += y.shape[0]\n",
    "    print(\"Class distribution is:\", attribute_count / samples_seen)\n",
    "    if n_tasks > 1:\n",
    "        task_class_weights = samples_seen / attribute_count - 1\n",
    "    else:\n",
    "        task_class_weights = np.array(\n",
    "            [attribute_count[0]/attribute_count[1]]\n",
    "        )\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "# Set log level in env variable as this will be necessary for\n",
    "# subprocessing\n",
    "os.environ['LOGLEVEL'] = os.environ.get(\n",
    "    'LOGLEVEL',\n",
    "    logging.getLevelName(logging.getLogger().getEffectiveLevel()),\n",
    ")\n",
    "loglevel = os.environ['LOGLEVEL']\n",
    "logging.info(f'Setting log level to: \"{loglevel}\"')\n",
    "\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "results = {}\n",
    "for split in range(\n",
    "    experiment_config['shared_params'].get(\"start_split\", 0),\n",
    "    experiment_config['shared_params'][\"trials\"],\n",
    "):\n",
    "    results[f'{split}'] = {}\n",
    "    now = datetime.now()\n",
    "    print(\n",
    "        f\"[TRIAL \"\n",
    "        f\"{split + 1}/{experiment_config['shared_params']['trials']} \"\n",
    "        f\"BEGINS AT {now.strftime('%d/%m/%Y at %H:%M:%S')}\"\n",
    "    )\n",
    "    # And then over all runs in a given trial\n",
    "    for current_config in experiment_config['runs']:\n",
    "        # Construct the config for this particular trial\n",
    "        trial_config = copy.deepcopy(experiment_config.get('shared_params', {}))\n",
    "\n",
    "        trial_config.update(current_config)\n",
    "        trial_config[\"concept_map\"] = concept_map\n",
    "        # Now time to iterate 5\n",
    "        # over all hyperparameters that were given as part\n",
    "        for run_config in generate_hyperatemer_configs(trial_config):\n",
    "            now = datetime.now()\n",
    "            run_config = copy.deepcopy(run_config)\n",
    "            evaluate_expressions(run_config)\n",
    "            run_config[\"extra_name\"] = run_config.get(\"extra_name\", \"\").format(\n",
    "                **run_config\n",
    "            )\n",
    "            old_results = None\n",
    "            full_run_name = (\n",
    "                f\"{run_config['architecture']}{run_config.get('extra_name', '')}\"\n",
    "            )\n",
    "            current_results_path = os.path.join(\n",
    "                result_dir,\n",
    "                f'{full_run_name}_split_{split}_results.joblib'\n",
    "            )\n",
    "            current_rerun = determine_rerun(\n",
    "                config=run_config,\n",
    "                rerun=rerun,\n",
    "                split=split,\n",
    "                full_run_name=full_run_name,\n",
    "            )\n",
    "            if current_rerun:\n",
    "                logging.warning(\n",
    "                    f\"We will rerun model {full_run_name}_split_{split} \"\n",
    "                    f\"as requested by the config\"\n",
    "                )\n",
    "            if (not current_rerun) and os.path.exists(current_results_path):\n",
    "                with open(current_results_path, 'rb') as f:\n",
    "                    old_results = joblib.load(f)\n",
    "\n",
    "            if run_config[\"architecture\"] in [\n",
    "                \"IndependentConceptBottleneckModel\",\n",
    "                \"SequentialConceptBottleneckModel\",\n",
    "            ]:\n",
    "                # Special case for now for sequential and independent CBMs\n",
    "                config = copy.deepcopy(run_config)\n",
    "                config[\"architecture\"] = \"ConceptBottleneckModel\"\n",
    "                config[\"sigmoidal_prob\"] = True\n",
    "                full_run_name = (\n",
    "                    f\"{config['architecture']}{config.get('extra_name', '')}\"\n",
    "                )\n",
    "                seq_old_results = None\n",
    "                seq_current_results_path = os.path.join(\n",
    "                    result_dir,\n",
    "                    f'Sequential{full_run_name}_split_{split}_results.joblib'\n",
    "                )\n",
    "                if os.path.exists(seq_current_results_path):\n",
    "                    with open(seq_current_results_path, 'rb') as f:\n",
    "                        seq_old_results = joblib.load(f)\n",
    "\n",
    "                ind_old_results = None\n",
    "                ind_current_results_path = os.path.join(\n",
    "                    result_dir,\n",
    "                    f'Sequential{full_run_name}_split_{split}_results.joblib'\n",
    "                )\n",
    "                if os.path.exists(ind_current_results_path):\n",
    "                    with open(ind_current_results_path, 'rb') as f:\n",
    "                        ind_old_results = joblib.load(f)\n",
    "                ind_model, ind_test_results, seq_model, seq_test_results = \\\n",
    "                    training.train_independent_and_sequential_model(\n",
    "                        task_class_weights=task_class_weights,\n",
    "                        n_concepts=n_concepts,\n",
    "                        n_tasks=n_tasks,\n",
    "                        config=config,\n",
    "                        train_dl=train_dl,\n",
    "                        val_dl=val_dl,\n",
    "                        test_dl=test_dl,\n",
    "                        split=split,\n",
    "                        result_dir=result_dir,\n",
    "                        rerun=current_rerun,\n",
    "                        project_name=project_name,\n",
    "                        seed=(42 + split),\n",
    "                        imbalance=imbalance,\n",
    "                        ind_old_results=ind_old_results,\n",
    "                        seq_old_results=seq_old_results,\n",
    "                        single_frequency_epochs=single_frequency_epochs,\n",
    "                        activation_freq=activation_freq,\n",
    "                    )\n",
    "\n",
    "                config[\"architecture\"] = \"IndependentConceptBottleneckModel\"\n",
    "                training.update_statistics(\n",
    "                    results[f'{split}'],\n",
    "                    config,\n",
    "                    ind_model,\n",
    "                    ind_test_results,\n",
    "                )\n",
    "                full_run_name = (\n",
    "                    f\"{config['architecture']}{config.get('extra_name', '')}\"\n",
    "                )\n",
    "                results[f'{split}'].update(\n",
    "                    intervention_utils.test_interventions(\n",
    "                        task_class_weights=task_class_weights,\n",
    "                        full_run_name=full_run_name,\n",
    "                        train_dl=train_dl,\n",
    "                        val_dl=val_dl,\n",
    "                        test_dl=test_dl,\n",
    "                        imbalance=imbalance,\n",
    "                        config=config,\n",
    "                        n_tasks=n_tasks,\n",
    "                        n_concepts=n_concepts,\n",
    "                        acquisition_costs=acquisition_costs,\n",
    "                        result_dir=result_dir,\n",
    "                        concept_map=concept_map,\n",
    "                        intervened_groups=intervened_groups,\n",
    "                        accelerator=accelerator,\n",
    "                        devices=devices,\n",
    "                        split=split,\n",
    "                        rerun=current_rerun,\n",
    "                        old_results=ind_old_results,\n",
    "                        independent=True,\n",
    "                        competence_levels=config.get(\n",
    "                        'competence_levels',\n",
    "                        [1],\n",
    "                    ),\n",
    "                    )\n",
    "                )\n",
    "                logging.debug(\n",
    "                    f\"\\tResults for {full_run_name} in split {split}:\"\n",
    "                )\n",
    "                for key, val in filter_results(\n",
    "                    results[f'{split}'],\n",
    "                    full_run_name,\n",
    "                    cut=True,\n",
    "                ).items():\n",
    "                    logging.debug(f\"\\t\\t{key} -> {val}\")\n",
    "                with open(ind_current_results_path, 'wb') as f:\n",
    "                    joblib.dump(\n",
    "                        filter_results(results[f'{split}'], full_run_name),\n",
    "                        f,\n",
    "                    )\n",
    "\n",
    "                config[\"architecture\"] = \"SequentialConceptBottleneckModel\"\n",
    "                training.update_statistics(\n",
    "                    results[f'{split}'],\n",
    "                    config,\n",
    "                    seq_model,\n",
    "                    seq_test_results,\n",
    "                )\n",
    "                full_run_name = (\n",
    "                    f\"{config['architecture']}{config.get('extra_name', '')}\"\n",
    "                )\n",
    "                results[f'{split}'].update(\n",
    "                    intervention_utils.test_interventions(\n",
    "                        task_class_weights=task_class_weights,\n",
    "                        full_run_name=full_run_name,\n",
    "                        train_dl=train_dl,\n",
    "                        val_dl=val_dl,\n",
    "                        test_dl=test_dl,\n",
    "                        imbalance=imbalance,\n",
    "                        config=config,\n",
    "                        n_tasks=n_tasks,\n",
    "                        n_concepts=n_concepts,\n",
    "                        acquisition_costs=acquisition_costs,\n",
    "                        result_dir=result_dir,\n",
    "                        concept_map=concept_map,\n",
    "                        intervened_groups=intervened_groups,\n",
    "                        accelerator=accelerator,\n",
    "                        devices=devices,\n",
    "                        split=split,\n",
    "                        rerun=current_rerun,\n",
    "                        old_results=seq_old_results,\n",
    "                        sequential=True,\n",
    "                        competence_levels=config.get('competence_levels', [1]),\n",
    "                    )\n",
    "                )\n",
    "                logging.debug(\n",
    "                    f\"\\tResults for {full_run_name} in split {split}:\"\n",
    "                )\n",
    "                for key, val in filter_results(\n",
    "                    results[f'{split}'],\n",
    "                    full_run_name,\n",
    "                    cut=True,\n",
    "                ).items():\n",
    "                    logging.debug(f\"\\t\\t{key} -> {val}\")\n",
    "                with open(seq_current_results_path, 'wb') as f:\n",
    "                    joblib.dump(\n",
    "                        filter_results(results[f'{split}'], full_run_name),\n",
    "                        f,\n",
    "                    )\n",
    "                if experiment_config['shared_params'].get(\"start_split\", 0) == 0:\n",
    "                    attempt = 0\n",
    "                    # We will try and dump things a few times in case there\n",
    "                    # are other threads/processes currently modifying or\n",
    "                    # writing this same file\n",
    "                    while attempt < 5:\n",
    "                        try:\n",
    "                            with open(\n",
    "                                os.path.join(result_dir, f'results.joblib'),\n",
    "                                'wb',\n",
    "                            ) as f:\n",
    "                                joblib.dump(results, f)\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            print(\n",
    "                                \"FAILED TO SERIALIZE RESULTS TO\",\n",
    "                                os.path.join(result_dir, f'results.joblib')\n",
    "                            )\n",
    "                            attempt += 1\n",
    "                    if attempt == 5:\n",
    "                        raise ValueError(\n",
    "                            \"Could not serialize \" +\n",
    "                            os.path.join(result_dir, f'results.joblib') +\n",
    "                            \" to disk\"\n",
    "                        )\n",
    "            else:\n",
    "                print(\"GETTING MODEL\")\n",
    "                model, model_results = \\\n",
    "                    training.train_model(\n",
    "                        task_class_weights=task_class_weights,\n",
    "                        accelerator=accelerator,\n",
    "                        devices=devices,\n",
    "                        n_concepts=n_concepts,\n",
    "                        n_tasks=n_tasks,\n",
    "                        config=run_config,\n",
    "                        train_dl=train_dl,\n",
    "                        val_dl=val_dl,\n",
    "                        test_dl=test_dl,\n",
    "                        split=split,\n",
    "                        result_dir=result_dir,\n",
    "                        rerun=current_rerun,\n",
    "                        project_name=project_name,\n",
    "                        seed=(42 + split),\n",
    "                        imbalance=imbalance,\n",
    "                        old_results=old_results,\n",
    "                        gradient_clip_val=run_config.get(\n",
    "                            'gradient_clip_val',\n",
    "                            0,\n",
    "                        ),\n",
    "                        single_frequency_epochs=single_frequency_epochs,\n",
    "                        activation_freq=activation_freq,\n",
    "                    )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "model_results"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pytorch_lightning as pl\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=accelerator,\n",
    "    devices=devices,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "trainer.test(model, test_dl)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"MODEL DEVICE: \", model.device)\n",
    "\n",
    "def make_predictions(model, dataloader, config):\n",
    "    all_x, all_groundtruths_y, all_groundtruths_c, all_predictions_c, all_predictions_y  = [], [], [], [], []\n",
    "    all_c_i_on_KL, all_c_i_off_KL = [], []\n",
    "\n",
    "    if config['architecture'] == 'ConceptEmbeddingModel':\n",
    "        all_positive_embeddings, all_negative_embeddings = [], []\n",
    "\n",
    "    all_correct = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model = model.to(device)\n",
    "\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if len(batch) == 2:\n",
    "                x_train, [y_train, c_train] = batch[0], batch[1]\n",
    "            else:\n",
    "                x_train, y_train, c_train = batch\n",
    "            \n",
    "            all_x.append(x_train)\n",
    "            all_groundtruths_y.append(y_train)\n",
    "            all_groundtruths_c.append(c_train)\n",
    "            \n",
    "            x_train, y_train, c_train = x_train.to(device), y_train.to(device), c_train.to(device)\n",
    "            batch = [x_train, y_train, c_train]\n",
    "\n",
    "            if config['architecture'] != 'ConceptEmbeddingModel':\n",
    "                outputs = model.predict_step(batch, batch_idx=i)\n",
    "                c_sem, c_pred, y_logits = outputs\n",
    "\n",
    "            else:\n",
    "                outputs = model.predict_step(batch, batch_idx=i, output_embeddings=True)\n",
    "                c_sem, c_pred, y_logits, pos_embeddings_batch, neg_embeddings_batch = outputs\n",
    "                all_positive_embeddings.append(pos_embeddings_batch.detach().cpu())\n",
    "                all_negative_embeddings.append(neg_embeddings_batch.detach().cpu())\n",
    "\n",
    "            all_predictions_c.append(c_sem.detach().cpu())\n",
    "            all_predictions_y.append(y_logits.detach().cpu())\n",
    "\n",
    "            if y_logits.size(1) > 1:\n",
    "                y_pred = torch.argmax(y_logits, dim=-1)\n",
    "            else:\n",
    "                y_pred = (torch.sigmoid(y_logits) >= 0.5).float().flatten()\n",
    "\n",
    "            correct = (y_train == y_pred).numpy(force=True)\n",
    "            all_correct.append(correct)\n",
    "\n",
    "            # get KL divergence after turning concepts on and off\n",
    "            if config['architecture'] != 'ConceptEmbeddingModel':\n",
    "                e = None\n",
    "            else:\n",
    "                e = {'positive_embeddings': pos_embeddings_batch.detach(), 'negative_embeddings': neg_embeddings_batch.detach()}\n",
    "\n",
    "            # concept_i_on_KL, concept_i_off_KL = c_on_and_off_KL_div(c_sem.detach(), model.c2y_model, embeddings=e)\n",
    "\n",
    "            # all_c_i_on_KL.append(concept_i_on_KL)\n",
    "            # all_c_i_off_KL.append(concept_i_off_KL)\n",
    "\n",
    "    all_correct = np.hstack(all_correct)\n",
    "    print(\"Accuracy (%) = \", np.mean(all_correct)*100)\n",
    "\n",
    "    predictions_dict = {\n",
    "        'x': all_x,\n",
    "        'groundtruths_y': torch.hstack(all_groundtruths_y),\n",
    "        'groundtruth_c': torch.vstack(all_groundtruths_c),\n",
    "        'predictions_y': torch.vstack(all_predictions_y),\n",
    "        'predictions_c': torch.vstack(all_predictions_c),\n",
    "        # 'concept_i_on_KL_div': torch.vstack(all_c_i_on_KL),\n",
    "        # 'concept_i_off_KL_div': torch.vstack(all_c_i_off_KL),\n",
    "    }\n",
    "\n",
    "    if config['architecture'] == 'ConceptEmbeddingModel':\n",
    "        predictions_dict['positive_embeddings'] = torch.vstack(all_positive_embeddings)\n",
    "        predictions_dict['negative_embeddings'] = torch.vstack(all_negative_embeddings)\n",
    "\n",
    "    return predictions_dict\n",
    "\n",
    "_ = make_predictions(model, test_dl, run_config)\n",
    "\n",
    "\n",
    "#                     training.update_statistics(\n",
    "#                         results[f'{split}'],\n",
    "#                         run_config,\n",
    "#                         model,\n",
    "#                         model_results,\n",
    "#                     )\n",
    "#                     results[f'{split}'].update(\n",
    "#                         intervention_utils.test_interventions(\n",
    "#                             task_class_weights=task_class_weights,\n",
    "#                             full_run_name=full_run_name,\n",
    "#                             train_dl=train_dl,\n",
    "#                             val_dl=val_dl,\n",
    "#                             test_dl=test_dl,\n",
    "#                             imbalance=imbalance,\n",
    "#                             config=run_config,\n",
    "#                             n_tasks=n_tasks,\n",
    "#                             n_concepts=n_concepts,\n",
    "#                             acquisition_costs=acquisition_costs,\n",
    "#                             result_dir=result_dir,\n",
    "#                             concept_map=concept_map,\n",
    "#                             intervened_groups=intervened_groups,\n",
    "#                             accelerator=accelerator,\n",
    "#                             devices=devices,\n",
    "#                             split=split,\n",
    "#                             rerun=current_rerun,\n",
    "#                             old_results=old_results,\n",
    "#                             competence_levels=run_config.get(\n",
    "#                                 'competence_levels',\n",
    "#                                 [1],\n",
    "#                             ),\n",
    "#                         )\n",
    "#                     )\n",
    "#                     results[f'{split}'].update(\n",
    "#                         training.evaluate_representation_metrics(\n",
    "#                             config=run_config,\n",
    "#                             n_concepts=n_concepts,\n",
    "#                             n_tasks=n_tasks,\n",
    "#                             test_dl=test_dl,\n",
    "#                             full_run_name=full_run_name,\n",
    "#                             split=split,\n",
    "#                             imbalance=imbalance,\n",
    "#                             result_dir=result_dir,\n",
    "#                             sequential=False,\n",
    "#                             independent=False,\n",
    "#                             task_class_weights=task_class_weights,\n",
    "#                             accelerator=accelerator,\n",
    "#                             devices=devices,\n",
    "#                             rerun=current_rerun,\n",
    "#                             seed=42,\n",
    "#                             old_results=old_results,\n",
    "#                         )\n",
    "#                     )\n",
    "\n",
    "#                     logging.debug(\n",
    "#                         f\"\\tResults for {full_run_name} in split {split}:\"\n",
    "#                     )\n",
    "#                     for key, val in filter_results(\n",
    "#                         results[f'{split}'],\n",
    "#                         full_run_name,\n",
    "#                         cut=True,\n",
    "#                     ).items():\n",
    "#                         logging.debug(f\"\\t\\t{key} -> {val}\")\n",
    "#                     with open(current_results_path, 'wb') as f:\n",
    "#                         joblib.dump(\n",
    "#                             filter_results(results[f'{split}'], full_run_name),\n",
    "#                             f,\n",
    "#                         )\n",
    "#                 if run_config.get(\"start_split\", 0) == 0:\n",
    "#                     attempt = 0\n",
    "#                     # We will try and dump things a few times in case there\n",
    "#                     # are other threads/processes currently modifying or\n",
    "#                     # writing this same file\n",
    "#                     while attempt < 5:\n",
    "#                         try:\n",
    "#                             with open(\n",
    "#                                 os.path.join(result_dir, f'results.joblib'),\n",
    "#                                 'wb',\n",
    "#                             ) as f:\n",
    "#                                 joblib.dump(results, f)\n",
    "#                             break\n",
    "#                         except Exception as e:\n",
    "#                             print(e)\n",
    "#                             print(\n",
    "#                                 \"FAILED TO SERIALIZE RESULTS TO\",\n",
    "#                                 os.path.join(result_dir, f'results.joblib')\n",
    "#                             )\n",
    "#                             attempt += 1\n",
    "#                     if attempt == 5:\n",
    "#                         raise ValueError(\n",
    "#                             \"Could not serialize \" +\n",
    "#                             os.path.join(result_dir, f'results.joblib') +\n",
    "#                             \" to disk\"\n",
    "#                         )\n",
    "#                 extr_name = run_config['c_extractor_arch']\n",
    "#                 if not isinstance(extr_name, str):\n",
    "#                     extr_name = \"lambda\"\n",
    "#                 then = datetime.now()\n",
    "#                 diff = then - now\n",
    "#                 diff_minutes = diff.total_seconds() / 60\n",
    "#                 logging.debug(\n",
    "#                     f\"\\tTrial {split + 1} COMPLETED for {full_run_name} ending \"\n",
    "#                     f\"at {then.strftime('%d/%m/%Y at %H:%M:%S')} \"\n",
    "#                     f\"({diff_minutes:.4f} minutes):\"\n",
    "#                 )\n",
    "#             print(f\"********** Results in between trial {split + 1} **********\")\n",
    "#             print_table(\n",
    "#                 config=experiment_config,\n",
    "#                 results=results,\n",
    "#                 result_table_fields=result_table_fields,\n",
    "#                 sort_key=sort_key,\n",
    "#                 result_dir=None,\n",
    "#                 split=split,\n",
    "#             )\n",
    "#             logging.debug(f\"\\t\\tDone with trial {split + 1}\")\n",
    "#     print(f\"********** Results after trial {split + 1} **********\")\n",
    "#     print_table(\n",
    "#         config=experiment_config,\n",
    "#         results=results,\n",
    "#         result_table_fields=result_table_fields,\n",
    "#         sort_key=sort_key,\n",
    "#         result_dir=result_dir,\n",
    "#         split=split,\n",
    "#     )\n",
    "#     logging.debug(f\"\\t\\tDone with trial {split + 1}\")\n",
    "#         # Locally serialize the results of this trial\n",
    "#     return results\n",
    "\n",
    "\n",
    "# ################################################################################\n",
    "# ## Arg Parser\n",
    "# ################################################################################\n",
    "\n",
    "\n",
    "# def _build_arg_parser():\n",
    "#     parser = argparse.ArgumentParser(\n",
    "#         description=(\n",
    "#             'Runs CEM intervention experiments in a given dataset.'\n",
    "#         ),\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         '--config',\n",
    "#         '-c',\n",
    "#         default=None,\n",
    "#         help=(\n",
    "#             \"YAML file with the configuration for the experiment. If not \"\n",
    "#             \"provided, then we will use the default configuration for the \"\n",
    "#             \"dataset.\"\n",
    "#         ),\n",
    "#         metavar=\"config.yaml\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         '--project_name',\n",
    "#         default='',\n",
    "#         help=(\n",
    "#             \"Project name used for Weights & Biases monitoring. If not \"\n",
    "#             \"provided, then we will not log in W&B.\"\n",
    "#         ),\n",
    "#         metavar=\"name\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         '--dataset',\n",
    "#         choices=[\n",
    "#             'cub',\n",
    "#             'celeba',\n",
    "#             'xor',\n",
    "#             'vector',\n",
    "#             'dot',\n",
    "#             'trig',\n",
    "#             'mnist_add',\n",
    "#             'chexpert',\n",
    "#             'derma',\n",
    "#         ],\n",
    "#         help=(\n",
    "#             \"Dataset to run experiments for. Must be a supported dataset with \"\n",
    "#             \"a loader.\"\n",
    "#         ),\n",
    "#         metavar=\"ds_name\",\n",
    "#         default=None,\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         '--output_dir',\n",
    "#         '-o',\n",
    "#         default=None,\n",
    "#         help=(\n",
    "#             \"directory where we will dump our experiment's results.\"\n",
    "#         ),\n",
    "#         metavar=\"path\",\n",
    "\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         '--rerun',\n",
    "#         '-r',\n",
    "#         default=False,\n",
    "#         action=\"store_true\",\n",
    "#         help=(\n",
    "#             \"If set, then we will force a rerun of the entire experiment even \"\n",
    "#             \"if valid results are found in the provided output directory. \"\n",
    "#             \"Note that this may overwrite and previous results, so use \"\n",
    "#             \"with care.\"\n",
    "#         ),\n",
    "\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         '--num_workers',\n",
    "#         default=8,\n",
    "#         help=(\n",
    "#             'number of workers used for data feeders. Do not use more workers '\n",
    "#             'than cores in the machine.'\n",
    "#         ),\n",
    "#         metavar='N',\n",
    "#         type=int,\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"-d\",\n",
    "#         \"--debug\",\n",
    "#         action=\"store_true\",\n",
    "#         default=False,\n",
    "#         help=\"starts debug mode in our program.\",\n",
    "#     )\n",
    "\n",
    "#     parser.add_argument(\n",
    "#         \"--force_cpu\",\n",
    "#         action=\"store_true\",\n",
    "#         default=False,\n",
    "#         help=\"forces CPU training.\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         '-p',\n",
    "#         '--param',\n",
    "#         action='append',\n",
    "#         nargs=2,\n",
    "#         metavar=('param_name=value'),\n",
    "#         help=(\n",
    "#             'Allows the passing of a config param that will overwrite '\n",
    "#             'anything passed as part of the config file itself.'\n",
    "#         ),\n",
    "#         default=[],\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         '--activation_freq',\n",
    "#         default=0,\n",
    "#         help=(\n",
    "#             'how frequently, in terms of epochs, should we store the '\n",
    "#             'embedding activations for our validation set. By default we will '\n",
    "#             'not store any activations.'\n",
    "#         ),\n",
    "#         metavar='N',\n",
    "#         type=int,\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         '--single_frequency_epochs',\n",
    "#         default=0,\n",
    "#         help=(\n",
    "#             'how frequently, in terms of epochs, should we store the '\n",
    "#             'embedding activations for our validation set. By default we will '\n",
    "#             'not store any activations.'\n",
    "#         ),\n",
    "#         metavar='N',\n",
    "#         type=int,\n",
    "#     )\n",
    "#     return parser\n",
    "\n",
    "\n",
    "# ################################################################################\n",
    "# ## Main Entry Point\n",
    "# ################################################################################\n",
    "\n",
    "# Build our arg parser first\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 ('CEM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a8e6f6cd250894acab83c7e1bc9e19a22dec4406cf4693b1ee8d8ab65502303"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
